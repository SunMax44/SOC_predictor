{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7c425b-eb04-4960-a588-69b3ade7f406",
   "metadata": {},
   "source": [
    "'''#old version, runs way too long!\n",
    "import ee\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize Earth Engine\n",
    "# Define service account and key file path\n",
    "SERVICE_ACCOUNT = ''\n",
    "KEY_PATH = ''\n",
    "# Authenticate and initialize Earth Engine API\n",
    "EE_CREDENTIALS = ee.ServiceAccountCredentials(SERVICE_ACCOUNT, KEY_PATH)\n",
    "ee.Initialize(EE_CREDENTIALS)\n",
    "\n",
    "# Load CSV file with soil sample data\n",
    "# Adjust the path to your CSV file\n",
    "data = pd.read_csv('/Users/maxsonntag/Documents/GitHub/SOC_predictor/data/date_location_lucas.csv')\n",
    "\n",
    "# Define cloud masking function\n",
    "def mask_clouds_sentinel2(image):\n",
    "    scl = image.select('SCL')\n",
    "    cloud_mask = scl.eq(3).Or(scl.eq(4)).Or(scl.eq(5)).Or(scl.eq(6))\n",
    "    return image.updateMask(cloud_mask)\n",
    "\n",
    "# Define function to calculate indices\n",
    "def calculate_indices_sentinel2(image):\n",
    "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    ndmi = image.normalizedDifference(['B8', 'B11']).rename('NDMI')\n",
    "    bsi = image.expression(\n",
    "        '((SWIR + RED) - (NIR + BLUE)) / ((SWIR + RED) + (NIR + BLUE))', {\n",
    "            'SWIR': image.select('B11'),\n",
    "            'RED': image.select('B4'),\n",
    "            'NIR': image.select('B8'),\n",
    "            'BLUE': image.select('B2')\n",
    "        }).rename('BSI')\n",
    "    soci = image.select('B2').divide(image.select('B3').multiply(image.select('B4'))).rename('SOCI')\n",
    "    return image.addBands([ndvi, ndmi, bsi, soci])\n",
    "\n",
    "# Define function to calculate trend (slope of the regression line)\n",
    "def calculate_trend(values):\n",
    "    if len(values) < 2 or any(v is None for v in values):\n",
    "        return None\n",
    "    x = np.arange(len(values)).reshape(-1, 1)\n",
    "    y = np.array(values).reshape(-1, 1)\n",
    "    model = LinearRegression().fit(x, y)\n",
    "    return model.coef_[0][0]\n",
    "\n",
    "# Function to fetch mean, std dev, and trend for indices over 5 years\n",
    "def fetch_statistics(lat, lon, sample_date):\n",
    "    # Parse date and define 5-year range\n",
    "    end_date = datetime.strptime(sample_date, '%Y-%m-%d')\n",
    "    start_date = end_date - timedelta(days=5*365)\n",
    "    \n",
    "    point = ee.Geometry.Point(lon, lat)\n",
    "    sentinel2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
    "                .filterDate(start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')) \\\n",
    "                .filterBounds(point) \\\n",
    "                .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 60)) \\\n",
    "                .map(mask_clouds_sentinel2) \\\n",
    "                .map(calculate_indices_sentinel2) \\\n",
    "                .select(['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12', \n",
    "                         'NDVI', 'NDMI', 'BSI', 'SOCI'])  # Select specific bands to ensure consistency\n",
    "    \n",
    "    # Create a mean composite image of the collection\n",
    "    composite = sentinel2.mean()\n",
    "    \n",
    "    indices = ['NDVI', 'NDMI', 'BSI', 'SOCI']\n",
    "    stats = {}\n",
    "\n",
    "    for index in indices:\n",
    "        # Use reduceRegion on the composite image\n",
    "        time_series = composite.select(index).reduceRegion(\n",
    "            reducer=ee.Reducer.toList(),\n",
    "            geometry=point.buffer(60),\n",
    "            scale=10,\n",
    "            maxPixels=1e6\n",
    "        ).get(index).getInfo()\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_value = np.nanmean(time_series)\n",
    "        stddev_value = np.nanstd(time_series)\n",
    "        trend_value = calculate_trend(time_series)\n",
    "        \n",
    "        stats[f'{index}_mean'] = mean_value\n",
    "        stats[f'{index}_stdDev'] = stddev_value\n",
    "        stats[f'{index}_trend'] = trend_value\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Process each row in the CSV file and append results\n",
    "results = []\n",
    "for _, row in data.iterrows():\n",
    "    date = row['sample_date']\n",
    "    lat = row['lat']\n",
    "    lon = row['long']\n",
    "    stats = fetch_statistics(lat, lon, date)\n",
    "    stats['date'] = date\n",
    "    stats['lat'] = lat\n",
    "    stats['long'] = lon\n",
    "    results.append(stats)\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv('/Users/maxsonntag/Documents/GitHub/SOC_predictor/data/indices_lucas.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8777224-dc18-49d3-ad97-cb7a9ccbb8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/concurrent/futures/process.py\", line 251, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'process_batch' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 110\u001b[0m\n\u001b[1;32m    107\u001b[0m                 f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(i))\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint updated. Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m run_batches_with_checkpointing()\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Combine all batch files into a single CSV after processing is complete\u001b[39;00m\n\u001b[1;32m    113\u001b[0m all_batches \u001b[38;5;241m=\u001b[39m [pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/maxsonntag/Documents/GitHub/SOC_predictor/data/sentinel_2/batches/output_batch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(batches))]\n",
      "Cell \u001b[0;32mIn[13], line 103\u001b[0m, in \u001b[0;36mrun_batches_with_checkpointing\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batches[start_batch:], start\u001b[38;5;241m=\u001b[39mstart_batch):\n\u001b[1;32m    102\u001b[0m     future \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39msubmit(process_batch, batch, i)\n\u001b[0;32m--> 103\u001b[0m     future\u001b[38;5;241m.\u001b[39mresult()  \u001b[38;5;66;03m# Wait for batch to complete\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Save the completed batch index to the checkpoint file\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "#new version, higher resolution, more efficient cloud masks / threshold, quarterly instead of monthly and using batches to run faster\n",
    "import ee\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Initialize Earth Engine\n",
    "# Define service account and key file path\n",
    "SERVICE_ACCOUNT = ''\n",
    "KEY_PATH = ''\n",
    "# Authenticate and initialize Earth Engine API\n",
    "EE_CREDENTIALS = ee.ServiceAccountCredentials(SERVICE_ACCOUNT, KEY_PATH)\n",
    "ee.Initialize(EE_CREDENTIALS)\n",
    "\n",
    "# Cloud masking function\n",
    "def mask_clouds_sentinel2(image):\n",
    "    scl = image.select('SCL')\n",
    "    cloud_mask = scl.eq(3).Or(scl.eq(4)).Or(scl.eq(5)).Or(scl.eq(6))\n",
    "    return image.updateMask(cloud_mask)\n",
    "\n",
    "# Indices calculation function\n",
    "def calculate_indices_sentinel2(image):\n",
    "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    ndmi = image.normalizedDifference(['B8', 'B11']).rename('NDMI')\n",
    "    bsi = image.expression(\n",
    "        '((SWIR + RED) - (NIR + BLUE)) / ((SWIR + RED) + (NIR + BLUE))', {\n",
    "            'SWIR': image.select('B11'),\n",
    "            'RED': image.select('B4'),\n",
    "            'NIR': image.select('B8'),\n",
    "            'BLUE': image.select('B2')\n",
    "        }).rename('BSI')\n",
    "    soci = image.select('B2').divide(image.select('B3').multiply(image.select('B4'))).rename('SOCI')\n",
    "    return image.addBands([ndvi, ndmi, bsi, soci])\n",
    "\n",
    "# Quarterly data retrieval function\n",
    "def fetch_quarterly_composites(lat, lon, sample_date):\n",
    "    end_date = datetime.strptime(sample_date, '%Y-%m-%d')\n",
    "    start_date = end_date - timedelta(days=5*365)\n",
    "    date_ranges = pd.date_range(start=start_date, end=end_date, freq='QS')\n",
    "    \n",
    "    point = ee.Geometry.Point(lon, lat)\n",
    "    data = []\n",
    "    \n",
    "    for start in date_ranges:\n",
    "        sentinel2 = (ee.ImageCollection('COPERNICUS/S2_SR')\n",
    "                     .filterDate(start.strftime('%Y-%m-%d'), (start + pd.DateOffset(months=3)).strftime('%Y-%m-%d'))\n",
    "                     .filterBounds(point)\n",
    "                     .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 60))  # 60% cloud threshold\n",
    "                     .map(mask_clouds_sentinel2)\n",
    "                     .map(calculate_indices_sentinel2))\n",
    "        \n",
    "        # Quarterly mean composite\n",
    "        quarterly_composite = sentinel2.mean()  # Mean composite\n",
    "\n",
    "        # Sample the indices at the location\n",
    "        result = quarterly_composite.reduceRegion(\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            geometry=point.buffer(60),  # 60m buffer\n",
    "            scale=30  # 30m resolution\n",
    "        ).getInfo()\n",
    "        \n",
    "        if result:\n",
    "            result['date'] = start.strftime('%Y-%m-%d')\n",
    "            data.append(result)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Function to process a single batch of locations\n",
    "def process_batch(batch, batch_index):\n",
    "    results = []\n",
    "    for _, row in batch.iterrows():\n",
    "        date = row['date']\n",
    "        lat = row['lat']\n",
    "        lon = row['long']\n",
    "        quarterly_data = fetch_quarterly_composites(lat, lon, date)\n",
    "        quarterly_data['lat'] = lat\n",
    "        quarterly_data['long'] = lon\n",
    "        results.append(quarterly_data)\n",
    "    \n",
    "    # Combine results for this batch and save to a separate file\n",
    "    batch_df = pd.concat(results, ignore_index=True)\n",
    "    batch_df.to_csv(f'/Users/maxsonntag/Documents/GitHub/SOC_predictor/data/sentinel_2/batches/output_batch_{batch_index}.csv', index=False)\n",
    "    print(f\"Batch {batch_index} processed and saved.\")\n",
    "\n",
    "# Load the initial CSV with 19,000 locations and set up checkpointing\n",
    "data = pd.read_csv('/Users/maxsonntag/Documents/GitHub/SOC_predictor/data/date_location_lucas.csv')\n",
    "batch_size = 500\n",
    "batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "\n",
    "# Check if a checkpoint file exists to resume from a specific batch\n",
    "try:\n",
    "    with open(\"checkpoint.txt\", \"r\") as f:\n",
    "        start_batch = int(f.read().strip()) + 1  # Start from the next batch\n",
    "except FileNotFoundError:\n",
    "    start_batch = 0  # Start from the first batch if no checkpoint exists\n",
    "\n",
    "# Run batches in parallel and save progress to checkpoint file after each batch\n",
    "def run_batches_with_checkpointing():\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        for i, batch in enumerate(batches[start_batch:], start=start_batch):\n",
    "            future = executor.submit(process_batch, batch, i)\n",
    "            future.result()  # Wait for batch to complete\n",
    "            \n",
    "            # Save the completed batch index to the checkpoint file\n",
    "            with open(\"checkpoint.txt\", \"w\") as f:\n",
    "                f.write(str(i))\n",
    "            print(f\"Checkpoint updated. Batch {i} completed.\")\n",
    "\n",
    "run_batches_with_checkpointing()\n",
    "\n",
    "# Combine all batch files into a single CSV after processing is complete\n",
    "all_batches = [pd.read_csv(f'/Users/maxsonntag/Documents/GitHub/SOC_predictor/data/sentinel_2/batches/output_batch_{i}.csv') for i in range(len(batches))]\n",
    "final_df = pd.concat(all_batches, ignore_index=True)\n",
    "final_df.to_csv('/Users/maxsonntag/Documents/GitHub/SOC_predictor/data/sentinel_2/indices_lucas.csv', index=False)\n",
    "print(\"All batches combined and saved to final_output.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68e327-e5da-4842-bf9d-35c942921dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
